from langchain.document_loaders import PyPDFLoader
import re
from habanero import Crossref
import arxiv
import unicodedata

def extract_text_from_pdf(path_to_pdf: str) -> str:
    """Extract and clean text from PDF using PyMuPDF (fitz)."""
    loader = PyPDFLoader(path_to_pdf)
    docs = loader.load()
    results = []
    for page in docs:
        text = page.page_content
        # --- Step 1: å»é™¤æ›è¡Œé€ æˆçš„æ–·è© & éå¤šæ›è¡Œ ---
        text = re.sub(r'-\s*\n\s*', '', text)  # æŠŠ "deep-\nfake" â†’ "deepfake"
        text = re.sub(r'\n+', ' ', text)       # å¤šå€‹æ›è¡Œ â†’ ç©ºæ ¼

        # --- Step 2: ç§»é™¤é çœ‰/é è…³å™ªéŸ³ ---
        text = re.sub(r'arXiv:\d+\.\d+(v\d+)?', ' ', text)
        text = re.sub(r'Page \d+/\d+', ' ', text)
        text = re.sub(r'Â©.*?(\d{4})', ' ', text)

        # --- Step 3: Unicode æ­£è¦åŒ– (å…¨å½¢â†’åŠå½¢, å¼•è™Ÿçµ±ä¸€ç­‰) ---
        text = unicodedata.normalize("NFKC", text)

        # --- Step 4: åˆªæ‰å­¤ç«‹æ•¸å­— (å¤šåŠæ˜¯é ç¢¼/åœ–è™Ÿ) ---
        text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)

        # --- Step 5: (é¸æ“‡æ€§) å°å¯«åŒ– ---
        # ğŸ‘‰ å»ºè­°åªå° query lowercaseï¼Œé€™è£¡æˆ‘ä¿ç•™åŸå§‹å¤§å°å¯«
        # text = text.lower()

        # --- Step 6: ç¬¦è™Ÿæ¸…ç† (å»æ‰æ²’èªæ„çš„åˆ†éš”ç·š) ---
        text = re.sub(r'[-=*_]{3,}', ' ', text)

        # --- å…¶ä»–å°ˆç”¨ pattern (æ•¸å­¸ç¬¦è™Ÿ/å…¬å¼/å¼•ç”¨/åœ–è¡¨) ---
        patterns = [
            r'\b\d+/\d+\b',                 # åˆ†æ•¸ (12/34)
            r'\b\d+\^\d+\b',                # æ¬¡æ–¹ (2^10)
            r'\b(?:sin|cos|tan|log|ln|exp)\b',  # å¸¸è¦‹å‡½æ•¸
            r'\[[0-9,\-\s]+\]',             # å¼•ç”¨ [12], [1-3]
            r'\bFig\.?\s*\d+|\bTable\s*\d+' # åœ–è¡¨
            r'\(\d+\)',
        ]
        # (A) å»æ‰æ•´æ®µå…¬å¼æ¨£å¼
        formula_patterns = [
            r'\$.*?\$',                # LaTeX inline formula: $...$
            r'\$\$.*?\$\$',            # LaTeX block formula: $$...$$
            r'\\\(.+?\\\)',            # \( ... \)
            r'\\\[.+?\\\]',            # \[ ... \]
            r'[A-Za-z0-9\s]*=[^,.;]+', # ç­‰è™Ÿå¾Œçš„é•·è¡¨é”å¼
        ]

        # (B) å»æ‰æ•¸å­¸å°ˆç”¨ç¬¦è™Ÿ
        symbol_patterns = [
            r'[â‰ˆâ‰¥â‰¤Â±âŠ—âˆ‘âˆ«âˆ‚âˆâˆ‡]',           # å¸¸è¦‹ç‰¹æ®Šç¬¦è™Ÿ
            r'[Î±-Ï‰Î‘-Î©]',               # å¸Œè‡˜å­—æ¯
            r'[\^_][{]?[A-Za-z0-9]+[}]?', # ä¸Šæ¨™/ä¸‹æ¨™
        ]

        # (C) å»æ‰ç´”æ•¸å­—å…¬å¼
        numeric_patterns = [
            r'\d+\s*[\+\-\*/^]\s*\d+',   # 2 + 2, 3*5, 2^10
            r'\d+\.\d+\s*%',             # ç™¾åˆ†æ¯” 30.4%
            r'âˆ¥[^âˆ¥]+âˆ¥',
            r'Ë†\s*\w*',
            r'\s*[,.:;]\s*[,.:;]+',
            r'^[,.:;]\s*|\s*[,.:;]$',
        ]
        for p in patterns+formula_patterns+symbol_patterns+numeric_patterns:
            text = re.sub(p, " ", text)

        # --- æœ€å¾Œ: ç§»é™¤å¤šé¤˜ç©ºç™½ ---
        text = re.sub(r'\s+', ' ', text).strip()
        results.append({'text': text, 'page': page.metadata.get('page', -1)})
    return results

# ---------- STEP 1: DOI ----------
def fetch_metadata_from_doi(doi: str) -> dict:
    cr = Crossref()
    try:
        result = cr.works(ids=doi)
        msg = result.get("message", {})
        title = msg.get("title", [None])[0]
        authors = [f"{a.get('given', '')} {a.get('family', '')}".strip()
                   for a in msg.get("author", [])]
        year = None
        if "published-print" in msg:
            year = msg["published-print"]["date-parts"][0][0]
        elif "issued" in msg:
            year = msg["issued"]["date-parts"][0][0]

        abstract = msg.get("abstract")
        if abstract:
            # CrossRef abstract å¯èƒ½å¸¶æœ‰ <jats:p> æ¨™ç±¤ï¼Œè¦æ¸…æ‰
            abstract = re.sub(r"<[^>]+>", "", abstract).strip()

        return {
            "doi": doi,
            "title": title,
            "authors": authors,
            "year": year,
            "abstract": abstract,
            "source": "crossref"
        }
    except Exception as e:
        return {"doi": doi, "source": "crossref_error", "error": str(e)}

# ---------- STEP 2: arXiv ----------
def fetch_metadata_from_arxiv(arxiv_id: str) -> dict:
    try:
        search = arxiv.Search(id_list=[arxiv_id])
        paper = next(search.results())
        return {
            "arxiv_id": arxiv_id,
            "title": paper.title,
            "authors": [str(a) for a in paper.authors],
            "year": paper.published.year,
            "abstract": paper.summary.strip(),
            "source": "arxiv"
        }
    except Exception as e:
        return {"arxiv_id": arxiv_id, "source": "arxiv_error", "error": str(e)}

# ---------- STEP 3: Heuristic fallback ----------
def extract_basic_metadata_from_pdf(file_path: str) -> dict:
    loader = PyPDFLoader(file_path)  # ç¢ºä¿æª”æ¡ˆå¯è®€
    docs = loader.load()
    first_page_text = docs[0].page_content

    # å˜—è©¦æŠ“ Title: å‡è¨­ç¬¬ä¸€è¡Œæ˜¯ title
    lines = [line.strip() for line in first_page_text.split("\n") if line.strip()]
    title = lines[0] if lines else None

    # å˜—è©¦æŠ“ Authors: å‡è¨­ç¬¬äºŒã€ä¸‰è¡Œå…§å« email æˆ–å¤§å¯«ç¸®å¯«
    authors = None
    for line in lines[1:5]:
        if "@" in line or re.search(r"[A-Z]\.\s*[A-Z]", line):
            authors = line
            break

    # å˜—è©¦æŠ“å¹´ä»½: å¾ References æˆ–é è…³
    year_match = re.search(r"(19|20)\d{2}", first_page_text)
    year = int(year_match.group(0)) if year_match else None

    # å˜—è©¦æŠ“ Abstract: æ‰¾ "Abstract" é–‹é ­æ®µè½
    abstract_match = re.search(r"(?i)abstract[:\s]*(.+?)(?=\n\s*[1I]\.|\n\s*Keywords|\n\s*Index|\Z)", 
                               first_page_text, re.S)
    abstract = abstract_match.group(1).strip() if abstract_match else None

    return {
        "title": title,
        "authors": authors,
        "year": year,
        "abstract": abstract,
        "source": "pdf_heuristic"
    }

# ---------- MASTER FUNCTION ----------
def extract_pdf_metadata(file_path: str) -> dict:
    loader = PyPDFLoader(file_path)  # ç¢ºä¿æª”æ¡ˆå¯è®€
    docs = loader.load()

    first_page_text = docs[0].page_content

    # Step 1: Try DOI
    doi_match = re.search(r"10\.\d{4,9}/[-._;()/:A-Z0-9]+", first_page_text, re.I)
    if doi_match:
        doi = doi_match.group(0)
        return fetch_metadata_from_doi(doi)

    # Step 2: Try arXiv ID
    arxiv_match = re.search(r"arXiv:\d{4}\.\d{4,5}(v\d+)?", first_page_text)
    if arxiv_match:
        arxiv_id = arxiv_match.group(0).replace("arXiv:", "")
        return fetch_metadata_from_arxiv(arxiv_id)

    # Step 3: Fallback Heuristic
    return extract_basic_metadata_from_pdf(file_path)
